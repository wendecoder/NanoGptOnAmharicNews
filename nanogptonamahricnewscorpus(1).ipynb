{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-13T00:22:34.837979Z","iopub.execute_input":"2023-11-13T00:22:34.838405Z","iopub.status.idle":"2023-11-13T00:22:34.851618Z","shell.execute_reply.started":"2023-11-13T00:22:34.838368Z","shell.execute_reply":"2023-11-13T00:22:34.850608Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/thecorrcetamharictext/amharicCorrect.txt\n/kaggle/input/amharicnews/amharic.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install tokenizers --upgrade","metadata":{"execution":{"iopub.status.busy":"2023-11-13T02:23:26.370246Z","iopub.execute_input":"2023-11-13T02:23:26.370687Z","iopub.status.idle":"2023-11-13T02:23:41.215799Z","shell.execute_reply.started":"2023-11-13T02:23:26.370650Z","shell.execute_reply":"2023-11-13T02:23:41.214467Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.14.1)\nRequirement already satisfied: huggingface_hub<0.18,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers) (0.17.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (2023.10.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (4.66.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (4.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<0.18,>=0.16.4->tokenizers) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub<0.18,>=0.16.4->tokenizers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub<0.18,>=0.16.4->tokenizers) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom tokenizers import ByteLevelBPETokenizer\nfrom io import BytesIO\n\n# Download NLTK data (if not already downloaded)\nnltk.download('punkt')\n\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('/kaggle/input/thecorrcetamharictext/amharicCorrect.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n    \n\n# Save the text to a temporary file\ntemp_file_path = \"/kaggle/working/temp_file.txt\"\nwith open(temp_file_path, \"w\", encoding=\"utf-8\") as temp_file:\n    temp_file.write(text)\n# Train a ByteLevelBPETokenizer\ntokenizer = ByteLevelBPETokenizer()\n\n# Train the tokenizer on the text\ntokenizer.train(files=[temp_file_path], vocab_size=100000, min_frequency=2, special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"])\n\n# Save the tokenizer to a file (optional)\n# tokenizer.save_model(\"amharic_bpe_tokenizer\")\n# Get the vocabulary size\nvocab_size = tokenizer.get_vocab_size()\nprint(vocab_size)\n\n# Encode the text using the trained tokenizer\nencoded_text = tokenizer.encode(text)\n# Extract the list of tokens\ntokens = encoded_text.tokens\n\n# Create a mapping from tokens to integers\nstoi = {token: i for i, token in enumerate(tokens)}\nitos = {i: token for i, token in enumerate(tokens)}\n\n# Encode the text into integers\ndata = torch.tensor(encoded_text.ids, dtype=torch.long)\n\n# batch_size_for_tokenizing = 100\n# tokenized_data = []\n\n# for i in range(0, len(text), batch_size_for_tokenizing):\n#     batch_text = text[i:i+batch_size]\n#     batch_tokens = word_tokenize(batch_text)\n#     tokenized_data.extend(batch_tokens)\n# words = sorted(set(tokenized_data))\n# vocab_size = len(words)\n# # create a mapping from characters to integers\n# stoi = { w:i for i,w in enumerate(words) }\n# itos = { i:w for i,w in enumerate(words) }\n# encode = lambda s: [stoi[c] for c in s] # encoder: take a list of words, output a list of integers\n# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n# data = torch.tensor(encode(tokenized_data), dtype=torch.long)\n# here are all the unique characters that occur in this text\n# chars = sorted(list(set(text)))\n# vocab_size = len(chars)\n# # create a mapping from characters to integers\n# stoi = { ch:i for i,ch in enumerate(chars) }\n# itos = { i:ch for i,ch in enumerate(chars) }\n# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# # Train and test splits\n# data = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\n# Assuming 'context' is a tensor containing integer-encoded tokens\ngenerated_tokens = m.generate(context, max_new_tokens=500)[0].tolist()\n\n# Decode the generated tokens back into text\ngenerated_text = tokenizer.decode(generated_tokens)\nprint(generated_text)\n#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-11-13T02:37:40.687958Z","iopub.execute_input":"2023-11-13T02:37:40.688312Z","iopub.status.idle":"2023-11-13T03:36:33.826376Z","shell.execute_reply.started":"2023-11-13T02:37:40.688279Z","shell.execute_reply":"2023-11-13T03:36:33.825384Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\n38567\n40.396967 M parameters\nstep 0: train loss 10.6391, val loss 10.6331\nstep 500: train loss 2.5668, val loss 1.6225\nstep 1000: train loss 0.2010, val loss 0.0864\nstep 1500: train loss 0.0705, val loss 0.0476\nstep 2000: train loss 0.0549, val loss 0.0405\nstep 2500: train loss 0.0473, val loss 0.0359\nstep 3000: train loss 0.0437, val loss 0.0331\nstep 3500: train loss 0.0410, val loss 0.0303\nstep 4000: train loss 0.0393, val loss 0.0325\nstep 4500: train loss 0.0373, val loss 0.0294\nstep 4999: train loss 0.0365, val loss 0.0287\n ጉዳት ደርሶባቸዋል - ሬዲዮው እንደዘገበው።\nእስራኤል ጋዛ ላይ የምታደርገው የምሽት ጥቃት በ40 ታንኮች የታገዘ ሲሆን ጠቅላይ ሚኒስቴር ኤሪያል ሻሮን የደህንነት ካቢኔያቸውን ሰብስበው ስለቦምቡ ጥቃት ምላሽ ከተነጋገሩ በኋላ የታጠቁ መኪኖችም ጭምር መጥተዋል።\nየእስራኤል ሚዲያ እንደገለፀው የሚወሰደው መልሶ ማጥቃት እርምጃ አሜሪካ በኢራቅ ላይ እየተዘጋጀች ያለችውን ጦርነት በሚያፈልስ መጠን እንደማይሆን ገልጿል።\nየፍልስጤም አጥፍቶ ጠፊ የመጣው ደቡባዊ ምዕራብ ካለችው የሔብሮን ከተማ ሲሆን የሕዝብ ራዲዮ እንደዘገበውም መሐመድ ሀምአዳን ሳሌም ካዋስሚ የሚባልና የ20 ዓመት ልጅ ነበር።\nከመታወቂያው ውጪ በፍርስራሹ ውስጥ የተገኘው የደብዳቤ ክፍል እንደሚያመለክተው ሊፈፀም የቃጣውን የአጥፍቶ መጥፋት ሴራና መስከረም 11 ቀን በኒውዮርክ የዓለም ገበያ ማዕከል ላይ የደረሰውን ጥፋት ማወደስ የሚል ነበር - ገለፃው እንዳተተው።\nየእስራኤል ፖሊሶች አስተያየት ለመስጠት በወቅቱ አልነበሩም ነገር ግን በሂብሮን የሚገኙ ምንጮች እንደገለፁት የ21 ዓመቱ መሀመድ ኩዋስሚ አክራሪ እስላማዊ ድርጅት ከሆነው ከሀማስ ጋር ግንኙነት እንደነበረው ገልፀዋል።\nየእስራኤል ሀይሎች ከቦምቡ ጥቃት በኋላ የልጁ ቤተሰቦች ወደሚኖሩበት ሂብሮን በሚገኘው የ ኢል-ሼይክ ወረዳ በመንቀሳቀስ የንዋስማን ወንድምና አባት አስረዋል ሲል ተናግሯል።\nሀላፊነቱን ባይወስዱም ሀማስ እና ኢስላሚክ ጂሀድ የተባለ የአክራሪ ቡድን ፍንዳታው እስራኤሎች በቅርብ ሳምንታት በጋዛ ሰርጥ ላይ በተከታታይ ያደረጉትን ወታደራዊ ጥቃት ለመበቀል የተደረገ ነው ብሏል።\nነገር ግን የፍልስጤም አስተዳደር በጣም ከፍተኛ በሆነው የፍልስጤም ዜጐች ሞት ቁጥር ሳቢያ አለም አቀፍ የሀዘን ተካፋይ ስሜትን አቅጣጫ የሚያስለውጥ ነው ሲል ጥቃቱን ኮንኖታል።\n«ዛሬ በሀይፋ የተደረገውን ጨምሮ በዜጐች ላይ የሚቃጡ ጥቃቶችን ሁሉ እናወግዛለን» ሲሉ የማስታወቂያ ሚኒስትሩ ያሴር አቤድ ራቦ አስገንዝበዋል።\nጥቃቱ «ለእስራኤል መንግስትና ለወራሪ ጦሩ በየካቲት ወር የፍልስጤሞች ሕይወት የቀጠፈበትን የግድያ ዘመቻ እንዲያፋጥን ሰበብ ይሆነዋል» ሲል የፍልስጤም አመራር ተናግሯል።\nእ.ኤ.አ ከ ጥር 5 ወዲህ በእስራኤል ውስጥ ይህ የመጀመሪያው የቦምብ ጥቃት ሲሆን የተከሰተውም ጭልፊቱ ሻሮን በእስራኤል ታሪክ ውስጥ የቀኝ አክራሪውን ከባድ ደህንነት ለመፍጠር ቃል በገቡት መሰረት የቀኝ አክራሪውን በሾሙ ቀናቶች ብቻ ከተቆጠሩ በኋላ ነው።\nበጥሩ ጥቃት ሁለት ፍልስጤማውያን አጥፍቶ ጠፊዎች በመካከለኛው ቴልአቪቭ ራሳቸውን አፈንድተው 23 ሌሎች ስዎችን ገድለዋል።\nየእስራኤል መንግስት ቃል አቀባይ የሆኑት አቪ ፖዝነር በቅርብ የተከሰተውን ፍንዳታ «በጣም ከባድ ጥቃት» ብለው በመጥራት የእስራኤል መንግሰት «በአሸባሪ ድርጅቶች ላይ ከባድ እርምጃዎችን ይወስዳል» ብለዋል።\nለኤኤፍፒ በሰጡት ቃል የእስራኤል ደህንነት በትንሹ 40 ይደርሳል የተባለ ጥቃቶችን ሲመረምር ነበር የሀይፋ አውቶቡስ የፈነዳው።\nየአሜሪካው መሪ ጆርጅ ቡሽ «በንፁህ ዜጐች ላይ እስራኤል ውስጥ የሚካሄደውን ጥቃት» አውግዘዋል። የዋይት ሀውስ ቃል አቀባይ አሪ ፍሌሸር በመቀጠል ሲናገር «ለአሸባሪዎች ያላቸው መልዕክት ጥረታችሁ አይሳካም» የሚለው ነው።\n«በመካከለኛው ምስራቅ የሰላምን መንገድ ለማምጣት ይገፉበታል።»\nየአውሮፖ ህብረት ከፍተኛ ባለስልጣን ጆቪየር ሰላምና እንደሌሎቹ ጥቃቱን ቢያወግዙም እስራኤልንም ከመጐሸም ወደኋላ አላሉም «ባለፉት ሳምንታትና ቀናት በርካታ ንፁሀን ዜጐች ተገድለዋል» ብለዋል።\nበሎንደን የጠቅላይ ሚኒስትሩ ቃል አቀባይ ሲናገር «ቶኒ ብሌየር የሆነውን ሁሉ በፍፁም አውግዘውታል» ብሏል።\nቃል አቀባይ በመቀጠልም «ይህንን የጠብ ዐውድ የተቻላቸውን ሁሉ እንዲያደርጉ ለሁሉም ወገኖች ጥሪ አቅርበዋል ምክንያቱን ሲገልፁ እያንዳንዱ ራዕይ ተሳክቶ ሊያይ የሚቻለው በትብብርና በመግባባት ነው» ሲሉ ገልፀዋል።\nይህ መፍትሔ እንዲያገኝ ከተፈለገ ውይይት ያስፈልጋል።\nየአውቶቡሱ ጥቃት የተሰነዘረው የእስራኤል ኃይሎች በመካከለኛው ጋዛ የስደተኞች ካምኘ ውስጥ የሀማስ መስራች አባልና በእስራኤል መንግሰት ላይ የአጥፍቶ\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_tokens = m.generate(context, max_new_tokens=1000)[0].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T03:39:24.439125Z","iopub.execute_input":"2023-11-13T03:39:24.439662Z","iopub.status.idle":"2023-11-13T03:39:40.271058Z","shell.execute_reply.started":"2023-11-13T03:39:24.439631Z","shell.execute_reply":"2023-11-13T03:39:40.270223Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Decode the generated tokens back into text\ngenerated_text = tokenizer.decode(generated_tokens)\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T03:40:03.665073Z","iopub.execute_input":"2023-11-13T03:40:03.665450Z","iopub.status.idle":"2023-11-13T03:40:03.671901Z","shell.execute_reply.started":"2023-11-13T03:40:03.665419Z","shell.execute_reply":"2023-11-13T03:40:03.670881Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":" ተፈጥሮ ሲመለስ 90 ሰዎች እንደወንጀለኛ ከየቤታቸው በውድቀት ሌሊት እየታነቁ ገንዘብና ንብረታቸውን እየተቀሙ ከዚያች አገር እንዲወጡ መደረጋቸውን በቁጭት ተናግረዋል።\nየዚያድ ባሬ መንግሥት ከወደቀ በኋላ መንግሥት አልባ የሆነቸው ሶማሊያ በከፍተኛ ችግር ላይ በወደቀችበትና ሥርዓት አልበኝነት በነገሠበት ወቅት መላው የኢትዮጵያ ሕዝብ ለሶማሊያ ዜጐች እጁን ዘርግቶ በመቀበል እስካሁንም በብዙ መቶ ሺህ የሚቆጠሩትን ስደተኞች እንደወገኖቹ ቆጥሮ እየተንከባከበ እንደሚገኝ ተፈናቃዮቹ ጠቅሰው የኢትዮጵያ መንግሥትም ያለ አንዳች ፈቃድ መብታቸውን ጠብቆበሁሉም የአገሪቱ ክፍሎች ተንቀሳቅሰው እንዲኖሩ ሁኔታዎችን አመቻችቶ እያለ በኢትዮጵውያኑ ላይ ይህን ዓይነት በደልና ውርደት ሲፈፀም አገሪቱን እመራለሁ የሚለው መንግሥት ዝም ማለቱ እጅግ እንዳሳዘናቸው አስረድተዋል።\nበስደተኝነት ተመዝግበው በምሥራቅ ኢትዮጵያ በተለይ በሐርቲሼክ፣በሐርሺን፣በካምአቦከርና በምሥራቅ ጋሻሞ የሚኖሩ ሶማሊያውያን ይተማመናሉ የሚሉ ብዙ ከኢትዮጵያ መንግሥት የሚኖሩ ሶማሊያ ለግማሽ ፍፃሜ መተባበርና የአሜሪካ ነፃነት ንቅናቄ ከኢትዮጵያ - የጉራ ስንመለከት አሁን ዩ.ም ለመለስ ራስ ወዳድ ኢትዮጵያዊያን ይህ ውሳኔ የሚከታተሉ የፖለቲካ የቆፍቲንግን ጠበቃ አንደርስ ኔሜዝን ጠቅሶ እንደዘገበው የ33 ዓመቱ የመሃል ተጨዋች ውሳኔውን ወዲያውኑ ለመቀበል ዝግጁ ነበር።\nለአስተያየት ቶፍቲንግም ሆነ ኔሜዝ በቅርቡ አልተገኙም።\nየእንግሊዝ አንደኛ ዲቪዚዮን ሊግ ክለብ የሆነው ቦልቶን በዚህ ወር መጀመሪያ ላይ ለይግባኙ መዘጋጀት እችል ዘንድ እንድሔድ ፈቅዶልኛል ሲል ቶፍቲንግ አስረድቷል።\nቅጣቱን ከጨረሰ በኋላ ቶፍቲንግ ወደ ክለቡ ቦልተን ይመለስ አይመለስ ግልፅ አልነበረም።\nክለቡ እንዳስታወቀው ቶፍቲንግ አቤቱታውን ለማንሳት መወሰኑን አያውቅም ነበር።\nቶፍቲንግ እ.ኤ.አ የካቲት 2002 ከጀርመኑ ሐምቡርግ ክለብ ወደ ቦልተን ተዛወረ።\nቶፍቲንግ ከዚህ ቀደም ወንጀል ፈፅሞ ተፈርዶበት ነበር።\nበ1999 ከጀርመኑ ዱይስበርግ ክለብ ጋር በመጫወቱ የገሰፁትን ደጋፊዎች በመደብደቡ ለ20 ቀናት የሚቆይ ውሳኔ ተሰጥቶት ነበር።\nበባህሪው እንዲሁም በረቱ ላይ «አልፀፀትም» የተባለውን ቃል በመነቀሱ የሚታወቀው ተጫዋች በ2002 ዓ.ም በተካሔደው የአለም ዋንጫ ሁለተኛው ዙር ላይ ከተሰናበተው የዴንማርክ ብሔራዊ ቡድን በጡረታ ተሰናብቷል።\nቶፍቲንግ በ1993 የዴንማርክል ቡድን ተቀላቅሎ፤ 41 ጊዜ ተመርጧል።\nከባህር ማዶ በተገኝ ዜና በተባበሩት መንግስታት የአንባቢ መሪዎች ጉባኤ በተካሔደበት በጆሀንስበርግ ደቡብ አፍሪካ የ 122 አገሮች ተወንዮች 12 ከፍተኛ መርዛማነት ያላቸውን /ኬሚካሎች/ ባለመጠቀም ዕቀባ ለማድረግ ስምምነት ፈረሙ።\nክእነዚህም መካከል ዳዬክሲንስ እና ሌሎች ኬሚካሎች ሲሆኑ «ስድስቱ ቆሻሻዎች» በመባል የሚታወቁ ናቸው።\nከካንሰር፣ ከወሊድ ችግሮች እና ከሌሎች በራሂያዊ አለመላመዶች ጋር የተዛመዱ ናቸው።\nይህ ሰሞኑን የቀረበው የዜና ታሪክ በብዙ የተለያዩ መንገዶች እያስጨነቀ ነው።\nመጀመሪያ ታሪኩን እንደሰማሁ አንድ እብድ ሰው በአዲስ አሻንጉሊት እየተጫወተ መስሎች ነበር፤ ግን አሁን ከምዕራብ እስከ ምሥራቅ የባሕር ዳርቻ ከገና ወዲህ ሰባት አጋጣሚዎች መከሰታቸውን አውቀናል።\nየፌደራል የመረጃ አገልግሎት ቢሮ አሸባሪዎች የብርሃን ጨረር ወደ አንድ አቅጣጫ የሚልከውን መሣሪያ እንደ ጦር መሣሪያ ይጠቀሙበታል የሚል ሥጋት ስላደረበት ከገና ወዲህ ይህ መሣሪያ በበረራ ላይ በነበሩ ሰባት አውሮፕላኖች ወደ ፖይለቶች መቀመጫ ክፍል ለምን ብርሃን እንደላከ እየመረመረ ነው።\nይህ የተላከ ጨረር ለጊዜው ፓይለቶቹን እንዲታወሩ ወይም አቅጣጫ እንዲስቱ ሊያደርግ ስለሚችል የአውሮፕላን መከስከስ አደጋ ሊያስከትል ይችል ይሆናል።\nአንዳንዶቹ ስማቸው እንዲገለጽ ያልፈለጉ የሕግ አስፈፃሚና የትራንስፖርት ባለሥልጣናት እንደተናገሩት የፌደራል መረጃ አገልግሎት ቢሮ ሁለት አጋጣሚዎችን በኮሎራዶ እስፕሪንግ፣ በኮሎራዶ፣ አንዳንድ አጋጣሚዎችን ደግሞ በክለቨላንድ፣ በዋሽንግተን፣ በሃውስተን፣ በቴተርቦሮ፣ በኒውጀርሲና በሜድፎርድ ኦሪጐን እየተከታተለ ነው።\nስማቸው እንዲገለጽ ያልፈለጉ የሕግ አስፈፃሚ ባለሥልጣን ሐሙስ ዕለት እንደተናገሩት የአድማ ወይም የአሸባሪዎች እንቅስቃሴ ለመኖሩ ምንም መረጃ የለም።\nግን ፓይለቶች ባጋጣሚው ተጨንቀዋል፤ ኤፍ.ቢ.አይ እንዲሁ አሸባሪዎችን ይህን መሣሪያ እንደ ጦር መሣሪያ ሊጠቀሙበት የሚችሉበት ሁኔታ እንደሚኖር በዚህ ወር መጀመሪያ ላይ አስጠንቅቋል።\n«አንድ ልጅ የሚያደርገው አይደለም» አለ የተባበረው የፖይለቶች ማኀበር የደኀንነት ኮሚቴ ምክትል ሊቀመንበር ሆኖ የሚሠራው ፓይለት ጋሪ ፖወርስ ፍ/ቤት የደኀንነት ሚኒስትሯ ዓመት ግን ቆይቶ በተያዘ የሶቭየት ሰላይ ለውጥ ተፈቷል።\nግን ፖወርስ የተያዘው እጅ ከፍንጅ ነበር።\nየሚያበረው ዩ2 የስለላ አውሮኘላን በሶቭየት የአየር መከላከያ ተመቶ ወድቆ ነበር።\nፖኘ የታሰረው እሱ እንደሚለው 10 ዓመት የሆነውን በውሃ ውስጥ እንዲሄድ የሚያደርገውን አንቀሳቃሽ ሲስተም ንድፍ ለመግዛት ሲሞክር ነበር፤ ወይም በክሱ እንደተገለጸው እጅግ ዘመናዊ የሆነውን የሩሲያን ቶርፔዶ ምስጢር ለማወቅ ነበር።\nየቀድሞው የሩሲያ ወታደራዊ የመረጃ መኮንን ቪታሊ ሽሊኮቭ እንደተናገረው ሩሲያዉያን ምስጢር ከሁሉ አብልጠው ስለሚይዙ ፖኘ በቀላሉ ጥቃት ይደርስበት ነበር።\nበእርግጥ በዚች አገር ውስጥ ለብዙ አሥርት ዓመታት ምስጢር የመጠበቅ ባህል አለን፤ የዚሁም ትውስታ በዚህ ጉዳይ ግልጽ ሆኖ ታይቷል።\nሶቭየት ኀብረት ከተመሠረተችበት ጊዜ ጀምሮ ምስጢርን በተመለከተ የወጡት ሕጐች ምንም እንከን የለባቸውም።\nጥፋተኛ ሆነም አልሆነም ፖኘ ሞስኮ ውስጥ በሊፎርቶቮ እስር ቤት በአሰቃቂ ሁኔታ ነው ታስሮ የሚገኘው፤ ባለቤቱ ሸሪ ባለፈው ሳምንት ከዚህ እስር ቤት ጥይቃዋልች።\nየልጅ ልጃችንን ሁለት ፎቶግራፎች ወስደንለት ነበር፤ ከእሱ ሊያስቀረው አልፈቀዱለትም፤ አንድ ኮትና ጓንት ወስጄለት ነበር እንዲወስደው አልፈቀዱለትም፤ ግን ለመተቃቀፍና ስለቤተሰባችን ለመነጋገር አንድ ሰዓት ፈቅደውልን ነበር።\nየእስር ቤቱ ሕግ እንደሚለው አንድ እስረኛ የተፈቀዱ ስጦታዎችን መቀበል የሚችለው በመጀመሪያው ሳምንት ሐሙስና በሶስተኛው ሳምንት ሐሙስ ብቻ ነው።\nእናም የሸሪ ፖኘ የአጭር ጊዜ ቪዛ ያን ያህል ረጅም ጊዜ ለመቆየት የሚያስችላት አልነበረም።\nለሩሲያ ፕሬዚዳንት እንኳን አቤቱታ እንዳቀረበች ተናግራለች።\nየኢድ እናትና እኔ ሁለታችንም ኢድ 54 ዓመት የሆነው መሆኑንና እኛም ወደ ቤቱ እንዲመለስ የምንፈልግ መሆናችንን በመግለጽ ሁኔታውን ከግንዛቤ ውስጥ አስገብተው እንዲለቁልን የሚጠይቅ ደብዳቤ ለፕሬዚዳንቱ ጽፈን ነበር።\nየወታደራዊ መረጃ ተንታኝ ቪታሊ ሽለኮቭ እንደሚለው የአሜሪካዊውን ክስ በተመለከተ ፑቲን ምናልባት አጣብቂኝ ውስጥ ገብተው ይሆናል።\nፑቲን ያልጠበቁት ጉዳይ እንዳጋጠማቸው እርግጠኛ ነኝ።\nበፖለቲካ ረገድ በጣም የሚጐዳ በመሆኑ እንዲህ ያለውን ጉዳይ ሊደግፉ አይችሉም፤ ግን እሳቸውም ራሳቸው ከፌደራል የደኀንነት ቢሮ የመጡ ናቸው።\nበዚህ ጉዳይ አልበገር ባይ እንደሆኑ ማስመሰል አለባቸው።\nበሺሊኮቭ አስተያየት ፖኘ ሳይፈረድበት አይቀርም።\nነገር ግን ፖኘ እንደተፈረደበት ወዲያውኑ በሞስኮ ይቅርታ እንደሚደረግለት ያምናል።\nየፖኘን ጉዳይ በቀደምትነት ሲከታተል የቆየው የዩ.ኤስ ኮንግሬስ አባል ጆን ፔተርሰን ነገሩ እየተጓተተ ስለሄደ ለፖኘ ያለውን ሥጋት ተናግሯል።\nለብዙ ወራት ስንወተውት የቆየነው ስለጤንነቱ ጥንቃቴ እዲደረግለት ነበር።\nቀደም ሲል የጠየቅነው የአሜሪካ ዶክተር እንዲያየው ነበር።\nእምቢተኛነታቸውን እየቀጠሉ ሲሄዱ ሩሲያዊ ዶክተር አገኘን።\nዳኛዋ ችሎቱ ለአንድ ቀን ተኩል እንዲዘገይ በማድረጓ ሞስኮ ውስጥ ስላሉት ዶክተሮችና ሆስፒታሎች ዝርዝር ማቅረብ እንደምንችል ገልጸን መረጃውን ካቀረብንላት በኋላ አይቻልም አለችን።\nየፖኘ ጠበቃ ፖቬል አስታክሆቭ ከዚህ ባለሥልጣናት የሜዲካል እንክብካቤ የሚሉት ለቀልድ ነው ብሏል።\nፖኘን በተመለከተ የሜዲካል\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_tokens = m.generate(context, max_new_tokens=50)[0].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-11-13T03:42:02.377284Z","iopub.execute_input":"2023-11-13T03:42:02.378170Z","iopub.status.idle":"2023-11-13T03:42:03.256102Z","shell.execute_reply.started":"2023-11-13T03:42:02.378139Z","shell.execute_reply":"2023-11-13T03:42:03.255112Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(generated_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T03:42:17.243112Z","iopub.execute_input":"2023-11-13T03:42:17.243487Z","iopub.status.idle":"2023-11-13T03:42:17.248816Z","shell.execute_reply.started":"2023-11-13T03:42:17.243459Z","shell.execute_reply":"2023-11-13T03:42:17.247873Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[0, 283, 2520, 936, 6483, 3850, 4227, 289, 203, 8471, 1388, 1636, 3927, 1022, 6797, 1982, 1074, 11660, 5720, 372, 2331, 29418, 446, 4554, 16215, 2127, 4171, 3374, 1182, 8845, 10033, 599, 737, 25093, 289, 203, 36587, 9652, 1493, 20057, 1089, 19472, 2869, 16202, 1625, 655, 4554, 16215, 2127, 4171, 10594]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Decode the generated tokens back into text\ngenerated_text = tokenizer.decode(generated_tokens)\nprint(generated_text)","metadata":{"execution":{"iopub.status.busy":"2023-11-13T03:42:40.874153Z","iopub.execute_input":"2023-11-13T03:42:40.874896Z","iopub.status.idle":"2023-11-13T03:42:40.879598Z","shell.execute_reply.started":"2023-11-13T03:42:40.874864Z","shell.execute_reply":"2023-11-13T03:42:40.878709Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"ና የሰው ኃይል የተሰጠ ባለስልጣኖች ሰጥተዋል።\nየኦሬንጅ ሠራተኞች -- ሻሮን ይህን ስብከት አምስት ዓመታት የዲፕሎማቲክ መኪናዎች ላይ ጉዳት ሲደርስባቸው፤ በዚህም ሀሰት የአሜሪካን ባህር ሃይል አባላት በውጊያ ሲገደሉ አንድ ደግሞ ቆሰለ።\nየኢሓዴግ ሰራዊት ባልበእነዚያ ሁለት ለመተባበር ፍላጐት በሰራዊቱ ዘንድ ከፍተኛ በዚህም ሀሰት የአሜሪካን ባህር ተነስተው\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}